# ğŸ›’ E-Commerce Sales Analytics Pipeline (PySpark + PostgreSQL)

This project builds an end-to-end **batch ETL pipeline** for e-commerce sales data using **PySpark** and **PostgreSQL**.  
It processes raw orders, customers, and product datasets and generates analytical aggregations, which are then loaded into PostgreSQL.

---

## ğŸ§  Overview

The pipeline processes three raw datasets:

- `orders.csv` (order details)
- `customers.json` (customer master)
- `products.json` (product catalog)

It then produces insights such as:

- Total revenue per category  
- Total revenue per customer  
- Top products by revenue  

The results are loaded into PostgreSQL tables created via `schemas.sql`.

---

## ğŸ—ï¸ Architecture

Raw Data (CSV + JSON)
â†“
PySpark Transformations
â†“
Aggregations (Category, Customer, Products)
â†“
PostgreSQL (agg_category, agg_customer, agg_top_products)


---

## ğŸ§° Tech Stack

- **PySpark** (Spark SQL + DataFrame API)
- **Python**
- **PostgreSQL**
- **psycopg2 + SQLAlchemy**
- **JSON + CSV data sources**

---

## ğŸ“‚ Project Structure

ecommerce_sales_pipeline/
â”‚
â”œâ”€â”€ data/
â”‚ â””â”€â”€ raw/
â”‚ â”œâ”€â”€ orders.csv
â”‚ â”œâ”€â”€ customers.json
â”‚ â””â”€â”€ products.json
â”‚
â”œâ”€â”€ scripts/
â”‚ â”œâ”€â”€ transform_spark.py # PySpark transformations & aggregations
â”‚ â”œâ”€â”€ load_postgres.py # Loads aggregated results into PostgreSQL
â”‚ â””â”€â”€ main.py # Orchestrator: runs transform â†’ load
â”‚
â”œâ”€â”€ config/
â”‚ â””â”€â”€ db_config.py # PostgreSQL credentials
â”‚
â”œâ”€â”€ schemas.sql # SQL schema for output tables
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md


---

## ğŸ§° Setup Instructions

```bash
# 1ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

# 2ï¸âƒ£ Start PostgreSQL and create required tables
psql -U postgres -d ecommerce_db -f schemas.sql

# 3ï¸âƒ£ Run PySpark Transformation
python scripts/transform_spark.py

# 4ï¸âƒ£ Load Aggregations into PostgreSQL
python scripts/load_postgres.py

# 5ï¸âƒ£ (Optional) Run Full Pipeline (Transform + Load)
python scripts/main.py



ğŸ—ƒï¸ PostgreSQL Output Tables

1. agg_category
Column	Description
category	Product category name
total_revenue	Total revenue for the category

2. agg_customer
Column	Description
customer_id	Unique customer ID
customer_name	Customer full name
customer_revenue	Total spend by that customer

3. agg_top_products
Column	Description
product_id	Product identifier
product_name	Product name
revenue	Total revenue generated

âš™ï¸ PySpark Transformations Performed

Read CSV + JSON inputs

Clean missing values

Join orders â†’ customers â†’ products

Compute:
total_amount = quantity Ã— price
category-level revenue
customer-level revenue
product-level revenue

Generate 3 aggregated DataFrames:
category insights
customer insights
top product insights

ğŸ’¾ Example Query Results

Top Categories
Electronics | 42000
Fashion     | 18500
Home        | 12900

Top Customers
Neha   | 5400
Ashok  | 4200
Simran | 3100
